# KARN
AI-Powered Sign Language Translator 
It is a website designed to capture the images, using camera and then identifies the data, converting it into text format.
It is a responsive website made using React,Python,Html,CSS, and Javascript.
Several Libraries of python are used ,which include: Torch,Sklearn,Numpy,Pandas,OpenCv,Mediapipe,TensorFlow etc.


Components of Karn:
1) Input:Capturing Sign Language(Image/Video Input)
2) Processing: Ai and Machine learning
   *)Computer Vision :-The system detects the hands, fingers, and body posture
    *)Deep Learning :- AI model analyzes the gesture
   *) Natural Language Processing :-Converts Recognized signs into text
3) Output: The signs are translated into texts
   
How AI recognizes sign Language:
1) Preprocessing:- The AI uses object detection (e.g., OpenCV, Mediapipe) to track hands.
It extracts features like finger positions, angles, and motion patterns.
2) Model Training:- Data is trained using dataset created by ourselves.
3) Gesture to text Conversion:- It translates it into English

Challenges & Future Developments

ğŸš§ Challenges:
Different sign languages exist (ASL, BSL, ISL, etc.).
Continuous gestures (not just individual signs) are harder to recognize.
Requires large, diverse datasets for accuracy.

ğŸš€ Future Improvements:
Real-time AI processing on mobile devices.
Integration with smart glasses (AR) for real-world applications.
Multilingual support to bridge communication barriers.

Challenges & Future Developments
ğŸš§ Challenges:

Different sign languages exist (ASL, BSL, ISL, etc.).
Continuous gestures (not just individual signs) are harder to recognize.
Requires large, diverse datasets for accuracy.

ğŸš€ Future Improvements:
Real-time AI processing on mobile devices.
Integration with smart glasses (AR) for real-world applications.
Multilingual support to bridge communication barriers.

Use Cases:-
âœ”ï¸ Accessibility for the Deaf & Hard of Hearing
âœ”ï¸ Real-time translation for meetings & education
âœ”ï¸ AI-powered sign language assistants

